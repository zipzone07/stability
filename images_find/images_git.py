# -*- coding: utf-8 -*-
"""images_git.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dgCKn1ZTCmGCHhmXJsd8Z2FstZUHOK3_

# Вступление

**Поиск изображений по запросу**

**Цель иследования:**

- разработать демонстрационную версию поиска изображений по запросу

**Ход исследования**

1. Загрузка  данных и и сследовательский анализ
2. Подготовка данных к обучению модели
3. Обучение модели
4. Тестирование модели и демонстрация ее работы
5. Общий вывод по работе

**Что важно учесть при выполнении проекта**

1. описать найденные в данных проблемы,
2. применить четкую  структуру проекта и придерживаться хорошего стиля при написании кода,
3. обозначить выводы по этапам,
4. кратко комментировать шаги,
5. использовать сводные таблицы для вывода данных,

# Исследовательский анализ данных

Данные содержат экспертные и краудсорсинговые оценки соответствия текста и изображения.

В файле с экспертными мнениями для каждой пары изображение-текст имеются оценки от трёх специалистов. Для решения задачи нужно агрегировать их — превратить в одну. Существует несколько способов агрегации оценок, самый простой — голосование большинства: за какую оценку проголосовала большая часть экспертов (в нашем случае 2 или 3), та оценка и ставится как итоговая. Поскольку число экспертов меньше числа классов, может случиться, что каждый эксперт поставит разные оценки, например: 1, 4, 2. В таком случае данную пару изображение-текст можно исключить из датасета.

В файле с краудсорсинговыми оценками информация расположена в таком порядке:

1. Доля исполнителей, подтвердивших, что текст **соответствует** картинке.
2. Количество исполнителей, подтвердивших, что текст **соответствует** картинке.
3. Количество исполнителей, подтвердивших, что текст **не соответствует** картинке.

После анализа экспертных и краудсорсинговых оценок выберем либо одну из них, либо объединим их в одну по какому-то критерию: например, оценка эксперта принимается с коэффициентом 0.6, а крауда — с коэффициентом 0.4.

Модель должна возвращать на выходе вероятность соответствия изображения тексту, поэтому целевая переменная должна иметь значения от 0 до 1.

Подготовка данных для анализа называется предобработкой. Нужно оценить масштаб найденных проблем и устранить их. Предобработка следует принципу GIGO
(от англ. garbage in — garbage out, буквально «мусор на входе — мусор на выходе»). Это значит, что при ошибках во входных данных даже правильный алгоритм работы приведёт к неверным результатам. Так же нужно помнить об законе экономии, который называется Бритва О́ккама. В нем говорится следующее: Не следует множить сущее без необходимости. Суть принципа: совершенство должно быть простым. Если какого-то результата можно достичь с привлечением сущностей A, B и C либо другим путём с привлечением A, B, С и D — надо выбирать первый путь. В плане программирования это значит, что не следует создавать промежуточные переменные, которые не пригодятся в дальнейшем.

## Загрузка данных
"""

!pip3 install requests -q
!pip3 install optuna -q
!pip3 install tensorflow -q
!pip3 install tqdm -q
!pip3 install lightgbm -q
!pip3 install nltk -q

import numpy as np
import pandas as pd
import numpy as np
import requests
import zipfile
import io
import re
import matplotlib.pyplot as plt
from sklearn.model_selection import (
    train_test_split,
    cross_val_score,
    GridSearchCV,
    GroupShuffleSplit
)
from sklearn.metrics import root_mean_squared_error, accuracy_score
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.pipeline import Pipeline
from tensorflow.keras.models import Sequential, load_model, Model
from tensorflow.keras.layers import (
    Conv2D,
    MaxPooling2D,
    Flatten,
    Dense,
    Dropout
)
from tensorflow.keras.layers import Dropout as dr
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras.preprocessing.image import (
    ImageDataGenerator,
    load_img,
    img_to_array
)
from keras.applications.resnet50 import ResNet50, preprocess_input
import scipy.sparse
from scipy.sparse import hstack
import spacy as sp
from nltk.corpus import stopwords as nltk_stopwords
from concurrent.futures import ProcessPoolExecutor
import lightgbm as lgb
from tqdm import tqdm
import warnings
import os
import random
from functools import lru_cache
from transformers import AutoTokenizer, AutoModel
import torch
from PIL import Image
from pathlib import Path
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# Вывод всех столбцов и строк, независимо от их количества.
pd.set_option('display.max_columns', None)

# URL ZIP-архива
url = 'https://code.s3.yandex.net/datasets/dsplus_integrated_project_4.zip'

# Скачивание архива
response = requests.get(url)
response.raise_for_status()

with io.BytesIO(response.content) as bytes_io:
    with zipfile.ZipFile(bytes_io) as z:
        # Распаковка
        z.extractall('/content/')
        # Переменные для хранения данных
        for filename in z.namelist():
            if filename.startswith('to_upload/'):
                base_name = filename.split('/')[-1]
                with z.open(filename) as file:
                    if base_name == 'CrowdAnnotations.tsv':
                        crow = pd.read_csv(
                            file,
                            sep='\t',
                            names=[
                                'image',
                                'query_id',
                                'perc_match',
                                'count_match',
                                'count_not_match'
                            ],
                            header=None
                        )
                    elif base_name == 'ExpertAnnotations.tsv':
                        expert = pd.read_csv(
                            file,
                            sep='\t',
                            names=[
                                'image',
                                'query_id',
                                'first_grade',
                                'second_grade',
                                'third_grade'
                            ],
                            header=None
                        )
                    elif base_name == 'test_images.csv':
                        test_images = pd.read_csv(file)
                    elif base_name == 'test_queries.csv':
                        test_queries = pd.read_csv(file, sep='|')
                    elif base_name == 'train_dataset.csv':
                        train_dataset = pd.read_csv(file)

# Вывод первых строк каждого файла
print('CrowdAnnotations')
display(crow.head())

print('ExpertAnnotations')
display(expert.head())

print('test_images')
display(test_images.head())

print('test_queries')
display(test_queries.head())

print('train_dataset')
display(train_dataset.head())

"""## Соответствие техническому описанию

<div style="border:ridge violet 5px; padding: 30px; border-radius: 15px;">
<h3> На заметку <a class="tocSkip"> </h3>
    
Признаковое описание для таблиц:

`CrowdAnnotations` — данные по соответствию изображения и описания, полученные с помощью краудсорсинга.

- 0 (image) - имя изображения
- 1 (query_id) - идентификатор описания, имеет формат: `<имя файла изображения>#<порядковый номер описания>`
- 2 (perc_match) - Доля людей, подтвердивших, что описание соответствует изображению
- 3 (count_match) - Количество человек, подтвердивших, что описание соответствует изображению
- 4 (count_not_match) - Количество человек, подтвердивших, что описание не соответствует изображению


`ExpertAnnotations` — данные по соответствию изображения и описания, полученные в результате опроса экспертов. Эксперты ставят оценки по шкале от 1 до 4, где:
- 1 — изображение и запрос совершенно не соответствуют друг другу,
- 2 — запрос содержит элементы описания изображения, но в целом запрос тексту не соответствует,
- 3 — запрос и текст соответствуют с точностью до некоторых деталей,
- 4 — запрос и текст соответствуют полностью.

Признаки:

- 0 (image) — имя изображения
- 1 (query_id) — идентификатор описания, имеет формат: `<имя файла изображения>#<порядковый номер описания>`
- 2 (first_grade) — оценки трёх экспертов
- 3 (second_grade) — оценки трёх экспертов
- 4 (third_grade) — оценки трёх экспертов

`test_images` — информация, необходимая для тестирования

- image - имя изображения

`test_queries` — информация, необходимая для тестирования. Для одной картинки может быть доступно до 5 описаний. Идентификатор описания имеет формат `<имя файла изображения>#<порядковый номер описания>`.

- query_id - идентификатор описания, имеет формат: `<имя файла изображения>#<порядковый номер описания>`
- query_text — текст запроса
- image - имя релевантного изображения

`train_dataset.csv` — информация, необходимая для обучения. Для одной картинки может быть доступно до 5 описаний

- image - имя изображения
- query_id - идентификатор описания, имеет формат: `<имя файла изображения>#<порядковый номер описания>`
- query_text — текст описания

Содержание архива:

`train_images` — в папке содержатся изображения для тренировки модели

`test_images` — в папке содержатся изображения для тестирования модели
"""

# Вывод информации
print('CrowdAnnotations')
info(crow)

print('ExpertAnnotations')
info(expert)

print('test_images')
info(test_images)

print('test_queries')
info(test_queries)

print('train_dataset')
info(train_dataset)

"""<div style="border:ridge violet 5px; padding: 30px; border-radius: 15px;">
<h3> На заметку <a class="tocSkip"> </h3>
    
Данные выглядят адекватно, пропусков нет.
"""

print('Количество уникальных изображений в тренировочном фрейме:',\
      train_dataset.image.nunique())
print('Количество уникальных изображений в тестовом фрейме:',\
      test_images.image.nunique())

print('Количество уникальных запросов в тренировочном фрейме:',\
      train_dataset.query_text.nunique())
print('Количество уникальных запросов в тестовом фрейме:',\
      test_queries.query_text.nunique())

# Сводные таблицы количества оценок для источников доверия
print('Экспертные оценки')
(pd.concat([
    # Группировка и определение количества оценок каждого из экспертов
    expert.groupby('first_grade').size().rename('Первый эксперт'),
    expert.groupby('second_grade').size().rename('Второй эксперт'),
    expert.groupby('third_grade').size().rename('Третий эксперт')
], axis=1)
.rename_axis(index='Тип оценки'))

print('Краудсординговые оценки')
crow.perc_match.value_counts()

"""<div style="border:ridge violet 5px; padding: 30px; border-radius: 15px;">
<h3> На заметку <a class="tocSkip"> </h3>
    
Экспертные оценки представляют из себя значения от 1 до 4, краудсординговые - это значения в диапазоне от 0 до 1. Оценки соответствуют признаковому описанию.
"""

# Просмотр тренировочных изображений
# Размер фигуры (ширина 13 дюймов, высота 13 дюймов)
fig = plt.figure(figsize=(13,13))

# Цикл итерации в диапазоне от 0 до 4
for i in range(5):
    # Новое окно-подграфик к общей фигуре
    # (1 ряд, 5 окон, номер окна начинается с 1)
    fig.add_subplot(1, 5, i+1)
    image = Image.open(Path(
        # Базовая папка
        '/content/to_upload/',
        # Набор изображений
        'train_images',
        # Случайный список из 5 изображений и берём i-е изображение
        list(train_dataset['image'].sample(5))[i]))

    # Изображение на экране
    plt.imshow(image)

    # Исключение отметок по осям
    plt.xticks([])
    plt.yticks([])

    # Оптимальное размещение элементов на странице
    plt.tight_layout()

# Просмотр тестовых изображений
fig = plt.figure(figsize=(13,13))

for i in range(5):
    fig.add_subplot(1, 5, i+1)
    image = Image.open(Path(
        '/content/to_upload/',
        'test_images',
        list(test_images['image'].sample(5))[i]))
    plt.imshow(image)
    plt.xticks([])
    plt.yticks([])
    plt.tight_layout()

"""<div style="border:ridge violet 5px; padding: 30px; border-radius: 15px;">
<h3> На заметку <a class="tocSkip"> </h3>
    
Просмотрев данные сделаем выводы:

1. Количество изображений по выборкам 1 к 10
2. Количество запросов примерно 1 к 2
3. Распределение оценок по источникам. У экспертов полное не соответствие запроса изображению примерно от 2350 до 4120 случаев, полное соответствие - от 247 до 413. У краудсординга с оценками по лучше и полное соответствие в 1323 случаях, полное не соответствие - 41970.
4. Картинки различного формата, часто встречаются изображения с детьми.

<div style="border:ridge violet 5px; padding: 30px; border-radius: 15px;">
<h3> На заметку <a class="tocSkip"> </h3>

Следуя заданию агрегируем оценки тоблицы `expert` из трех в одну методом голосования большинства. За какую оценку проголосовала большая часть экспертов (в нашем случае 2 или 3), та оценка и ставится как итоговая. Если нет одинаковых оценок, ставим пропуск.
"""

# Функция для голосования большинства
def expert_assessment(row):
    grades = [row['first_grade'], row['second_grade'], row['third_grade']]
    # Частота
    counts = pd.Series(grades).value_counts()
    # Поиск оценок с количеством >=2
    majority = counts[counts >= 2]
    if len(majority) == 1:
        # Есть единственная оценка с большинством
        return majority.index[0]
    else:
        # Нет явного большинства
        return None

# Приминение функции и создание нового столбца
expert['grade'] = expert.apply(expert_assessment, axis=1)

# Проверка резульата
expert.grade.value_counts()

# Проверка резульата
expert.isna().sum()

"""<div style="border:ridge violet 5px; padding: 30px; border-radius: 15px;">
<h3> На заметку <a class="tocSkip"> </h3>

В результате голосования образовалось 126 пропусков. Далее удалим лишние столбцы `first_grade, second_grade, third_grade`.
"""

# Выбор только нужных столбцов
expert = expert[['image', 'query_id', 'grade']]

"""<div style="border:ridge violet 5px; padding: 30px; border-radius: 15px;">
<h3> На заметку <a class="tocSkip"> </h3>

Учитывая доверие к разным источникам оценки объединим экспертные и краудсординговые оценки. Для краудсординговых оценок уже есть доля подтверждающих соответствие в столбце `perc_match`, используем их на прямую. Если доля людей, подтвердивших, что описание соответствует изображению равна 1, то вес `weight_crow` равен 0.4. В другом случае 0

Для экспертных мнений на основе полученных оценок в столбце `grade` выделим вес `weight_exp` следующим образом. Если экспертная оценка 0 — вес принимает значение 0, если оценка 1 — вес принимает значение 0.33 и так далее.

$$\text{weight}\left(\frac{g-1}{3}\right) =
\begin{cases}
0 & \text{если } g=1 \\
0.33 & \text{если } g=2 \\
0.66 & \text{если } g=3 \\
1 & \text{если } g=4
\end{cases}$$
"""

# Использование лямбда функции для фрейма экспертов
expert['weight_exp'] = expert['grade'].apply(
    lambda x: 0 if x == 1 else 0.33 if x == 2 else 0.66 if x == 3 else 1
)

# Баланс весов
expert['weight_exp'].value_counts()

"""<div style="border:ridge violet 5px; padding: 30px; border-radius: 15px;">
<h3> На заметку <a class="tocSkip"> </h3>

Объединим фреймы с оценками по столбцам `image` и `query_id` используя `pd.merge()` с сохранением строк из фрейма `expert` и `crow`, так как нужно учесть доверие всех источников.
"""

crow_exp_match = pd.merge(expert, crow, on=['image', 'query_id'], how='outer')
crow_exp_match.sample(5)

"""<div style="border:ridge violet 5px; padding: 30px; border-radius: 15px;">
<h3> На заметку <a class="tocSkip"> </h3>

Теперь создадим целевую переменную `match` на основе столбца `weight_exp` с заполнением образовавшихся пропусков при объединении данными из столбца `perc_match`
"""

crow_exp_match['match'] = round(
    crow_exp_match['weight_exp']
    .fillna(crow_exp_match['perc_match']),
    2
)
crow_exp_match.sample(5)

# Количество пропусков в целевой переменной
crow_exp_match.isna().sum()

"""<div style="border:ridge violet 5px; padding: 30px; border-radius: 15px;">
<h3> На заметку <a class="tocSkip"> </h3>

Проверим распределение оценок и удалим лишний столбец в таблице `test_queries`
"""

# Баланс весов
crow_exp_match['match'].value_counts()

test_queries = test_queries.drop('Unnamed: 0', axis=1)

"""<div style="border:ridge violet 5px; padding: 30px; border-radius: 15px;">
<h3> На заметку <a class="tocSkip"> </h3>

Создадим копии основных фреймов для сохраненния исходных данных. Следующим этапом объединим фрейм с оценками `crow_exp_match` и тренировочные данные `train_dataset` по столбцам `image` и `query_id`.
"""

# Объединение с охранением срок тренировочной выборки
train = train_dataset.copy()
test = test_queries.copy()
train = (
    pd.merge(train, crow_exp_match, on=['image', 'query_id'], how='left')
)

# Количество пропусков в целевой переменной
train.isna().sum()

"""<div style="border:ridge violet 5px; padding: 30px; border-radius: 15px;">
<h3> На заметку <a class="tocSkip"> </h3>

Визуализируем распределение оценок
"""

plt.figure(figsize=(8, 6))
train['match'].value_counts().plot(kind='bar')
plt.title('Гистограмма оценок')
plt.xlabel('Оценки')
plt.ylabel('Количество оценок')
plt.show()

"""# Подготовка данных к обучению модели

В некоторых странах, где работает компания, действуют ограничения по обработке изображений: поисковым сервисам и сервисам, предоставляющим возможность поиска, запрещено без разрешения родителей или законных представителей предоставлять любую информацию, в том числе, но не исключительно тексты, изображения, видео и аудио, содержащие описание, изображение или запись голоса детей. Ребёнком считается любой человек, не достигший 16 лет.

В сервисе строго следуют законам стран, в которых работают. Поэтому при попытке посмотреть изображения, запрещённые законодательством, вместо картинок показывается дисклеймер:

> This image is unavailable in your country in compliance with local laws
>

Однако в PoC нет возможности воспользоваться данным функционалом. Поэтому все изображения, которые нарушают данный закон, нужно удалить из обучающей выборки.

<div style="border:ridge violet 5px; padding: 30px; border-radius: 15px;">
<h3> На заметку <a class="tocSkip"> </h3>

Создадим мешок слов, относящиеся к детям до 16 лет для дальнейшего исключения из обучающего датасета пар, которые, могут попадать под юридические ограничения.
"""

bag_word_lim = pd.Series(
    [
        'child',
        'children',
        'kid',
        'kids',
        'toddler',
        'toddlers',
        'infant',
        'infants',
        'baby',
        'babies',
        'preteen',
        'preteens',
        'teenager',
        'teenagers',
        'teen',
        'teens',
        'adolescent',
        'adolescents',
        'youth',
        'youths',
        'juvenile',
        'juveniles',
        'youngster',
        'youngsters',
        'school-age child',
        'school-age children',
        'schoolboy',
        'schoolboys',
        'schoolgirl',
        'schoolgirls',
        'elementary school student',
        'elementary school students',
        'middle school student',
        'middle school students',
        'high school student',
        'high school students',
        'pre-adolescent',
        'pre-adolescents',
        'puberty',
        'minor',
        'minors',
        'juvenile',
        'juveniles',
        'baby shower',
        'playground',
        'playgrounds',
        'kindergarten class',
        'kindergarten classes',
        'preschool activity',
        'preschool activities',
        'youth sports',
        'youth sport',
        'teenagers',
        'family',
        'families'
            ]
)

"""## Векторизация текстов

Следующий этап — векторизация текстов. Исключим из обучающего датасета пары, которые, исходя из подготовленного списка слов, могут попадать под юридические ограничения.

<div style="border:ridge violet 5px; padding: 30px; border-radius: 15px;">
<h3> На заметку <a class="tocSkip"> </h3>

Чтобы дальше двигать в проекте необходимо лемматизировать всю текстовую информацию, мешок с словами юридических ограничений, тренировочный фрейм с помощью библиотеки `SpaCy`
"""

# Функция для лемматизации
nlp = sp.load('en_core_web_sm', disable=['parser', 'ner'])

def lemmatize_text(text):
    doc = nlp(text)
    return " ".join([token.lemma_ for token in doc if token.is_alpha])

# ProcessPoolExecutor для параллельной обработки лемматизации
with ProcessPoolExecutor() as executor:
    train['clear_query_text'] = list(
        executor
        .map(lemmatize_text, train['query_text'])
    )
    test['clear_query_text'] = list(
        executor
        .map(lemmatize_text, test['query_text'])
    )

"""<div style="border:ridge violet 5px; padding: 30px; border-radius: 15px;">
<h3> На заметку <a class="tocSkip"> </h3>

Теперь очистим тренировочный датасет от фотографий, попадающих под юридические ограничения
"""

# Функция, которая проверяет наличие хотя бы одного
# слова из списка bag_word_lim в переданном тексте
def contains_forbidden_word(text, bag_word_lim):
    # Текст в строку и к нижнему регистру
    for word in bag_word_lim:
        # Регулярное выражение для поиска целого слова (\b для границ слова)
        pattern = r'\b' + re.escape(word.lower()) + r'\b'
        # Совпадение в тексте с помощью регулярного выражения
        if re.search(pattern, text):
            # Если найдено хотя бы одно слово из списка, возврат True
            return True
    # Если ни одно слово не найдено, возврат False
    return False

# Булева маска: для каждого текста в столбце
# 'clear_query_text' приминение функции,
# которая возвращает True, если в тексте есть хотя бы одно запрещенное слово
mask_train = (
    train['clear_query_text']
    .apply(lambda x: contains_forbidden_word(x, bag_word_lim))
)

# Фильтрация DataFrame: только те строки, где mask == False
# и сброс индексов после фильтрации
train = train[~mask_train].reset_index(drop=True)

print('Очищенные данные:', len(train))
print('Исходные данные:', len(train_dataset))

"""<div style="border:ridge violet 5px; padding: 30px; border-radius: 15px;">
<h3> На заметку <a class="tocSkip"> </h3>

В тренировочных данных под очистку попало 581 изображение. Следующий этап — векторизация текстов. Применим метод `BERT`
"""

# Импорт класса AutoTokenizer и токенизатор для модели 'bert-base-uncased'
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

# Импорт класса AutoModel и предобученную модель BERT
model = AutoModel.from_pretrained('bert-base-uncased')

# Функция get_vector_text с использованием кеширования результатов
# для ускорения повторных вызовов
@lru_cache(maxsize=None)
def get_vector_text(text, model=model, tokenizer=tokenizer):
    # Токенизация входного текста:
    # - padding=True: паддинг до max_length
    # - truncation=True: обрезка текста, если он длиннее max_length
    # - max_length=64: максимальная длина последовательности токенов
    # - return_tensors='pt': возвращает тензоры PyTorch
    token = tokenizer(
        text,
        padding=True,
        truncation=True,
        max_length=64,
        return_tensors='pt'
    )

    # Модель в режим оценки
    model.eval()

    # Отключение вычисления градиентов для ускорения и экономии памяти
    with torch.no_grad():
        # Токенизированный вход в модель и выходные данные
        model_output = model(**token)

    # Извлечение пуллерного слоя
    embedding = model_output.pooler_output

    # Возврат первого элемента батча (так как у нас один текст),
    # преобразованный в NumPy массив
    return (embedding)[0].numpy()

# Функция get_vector_text к каждому элементу столбца
# 'query_text' датафрейма df_train,
# и новый столбец 'text_vector' с эмбеддингами текста
train['text_vector'] = train['query_text'].apply(get_vector_text)
test['text_vector'] = test['query_text'].apply(get_vector_text)

print('Длина эмбеддинга (плотного вектора) train:',\
      len(list(train['text_vector'].iloc[50])))
print('Длина эмбеддинга (плотного вектора) test:',\
      len(list(test['text_vector'].iloc[50])))

"""## Векторизация изображений

Перейдём к векторизации изображений.

Самый примитивный способ — прочесть изображение и превратить полученную матрицу в вектор. Такой способ не подходит: длина векторов может быть сильно разной, так как размеры изображений разные. Поэтому стоит обратиться к свёрточным сетям: они позволяют "выделить" главные компоненты изображений. Как это сделать? Нужно выбрать какую-либо архитектуру, например ResNet-50, посмотреть на слои и исключить полносвязные слои, которые отвечают за конечное предсказание. При этом можно загрузить модель данной архитектуры, предварительно натренированную на датасете ImageNet.

<div style="border:ridge violet 5px; padding: 30px; border-radius: 15px;">
<h3> На заметку <a class="tocSkip"> </h3>

Для векторизации изображений используем сверточную сеть `ResNet50`, предварительно обученную на наборе данных `ImageNet` с отключенными полносвязными слоямим по предсказанию
"""

# Объект ImageDataGenerator для аугментации и пред. обработки изображений
train_datagen = ImageDataGenerator(
    # Масштабирование пикселей изображений в диапазон,
    # подходящий для модели
    preprocessing_function=preprocess_input
)

# Генератор данных из DataFrame
train_gen_flow = train_datagen.flow_from_dataframe(
    # Передача DataFrame с данными о изображениях и метках
    dataframe=train,
    directory='/content/to_upload/train_images/',
    # Название столбца в DataFrame
    x_col='image',
    # Название столбца с метками
    y_col='match',
    # Размер изображений после изменения
    target_size=(224, 224),
    # Размер пакета данных (сколько изображений будет подаваться за один раз)
    batch_size=32,
    # Режим определения меток: 'raw' означает,
    # что метки передаются как есть (например, числа или строки)
    class_mode='raw',
    # Этот генератор используется для обучающей части данных
    # (выделенной из validation_split)
    subset='training',
    # бЕЗ перемешиваниЯ данных, чтобы порядок изображений оставался неизменным
    # (важно для предсказаний)
    shuffle=False,
    # УвоспроизводимостЬ случайных процессов при разделении данных
    seed=12345
)

test_datagen = ImageDataGenerator(
    preprocessing_function=preprocess_input
)

test_gen_flow = test_datagen.flow_from_dataframe(
    dataframe=test_images,
    directory='/content/to_upload/test_images/',
    x_col='image',
    target_size=(224, 224),
    batch_size=32,
    # метки не нужны для тестовых данных (только для предсказаний)
    class_mode=None,
    shuffle=False,
    seed=12345
)

"""<div style="border:ridge violet 5px; padding: 30px; border-radius: 15px;">
<h3> На заметку <a class="tocSkip"> </h3>

Проверим корректно ли загрузились изображения
"""

# Пследующий батч данных из генератора train_gen_flow
images, labels_batch = next(train_gen_flow)
# Первые 5 изображений из батча
images = images[:5]
# Соответствующие метки для этих изображений
labels_batch = labels_batch[:5]

# Фигура для отображения изображений с заданным размером
plt.figure(figsize=(15, 8))
# Первые 15 изображениям
for i in range(5):
    plt.subplot(3, 5, i + 1)
    plt.imshow(images[i])
    # Заголовок с меткой, которая соответствует запросу (класс или значение)
    plt.title(f'Соответствие запросу: {labels_batch[i]}')
    # Без осей
    plt.axis('off')
plt.tight_layout()
plt.show()

images = next(test_gen_flow)
images = images[:5]
labels_batch = labels_batch[:5]

plt.figure(figsize=(15, 8))
for i in range(5):
    plt.subplot(3, 5, i + 1)
    plt.imshow(images[i])
    plt.axis('off')
plt.tight_layout()
plt.show()

"""<div style="border:ridge violet 5px; padding: 30px; border-radius: 15px;">
<h3> На заметку <a class="tocSkip"> </h3>

Изображения загрузились корректно, далее используем `ResNet50` для получения векторов изображений
"""

def create_image_vectors(
    train_df,
    generator,
    label_column='match',
    vector_column='image_vector'
):
    """
    Выполняет извлечение признаков изображений с помощью предобученной ResNet50
    и добавляет их в DataFrame как новый столбец.

    Parameters:
        train_df (pd.DataFrame): исходный DataFrame с данными.
        generator: глобальный генератор, настроенный на train_df.
        label_column (str): название столбца с метками.
        vector_column (str): название нового столбца для векторов признаков.

    Возвращает:
        pd.DataFrame: обновленный DataFrame с добавленным столбцом 'image_vector'.
    """
    # Модель ResNet50 без верхних слоёв (классификационных),
    # с глобальным усреднением признаков
    base_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')

    # Список для хранения извлеченных признаков
    features = []

    # Общее количество образцов из исходного DataFrame
    total_samples = len(train_df)

    # Размер батча из настроек генератора
    batch_size = generator.batch_size

    # Итерация по всему набору данных батчами
    for i in range(int(np.ceil(total_samples / batch_size))):
        # Следующий батч изображений и меток из генератора
        batch_images, i = next(generator)

        # Предобработка изображений для модели ResNet50 (нормализация)
        batch_images = preprocess_input(batch_images)

        # Признаки для текущего батча изображений через модель
        batch_features = base_model.predict(batch_images)

        # Полученные признаки в список
        features.append(batch_features)

        # Проверка, достигли ли мы конца данных
        # (по количеству обработанных изображений)
        if len(features) * batch_size >= total_samples:
            # Выход из цикла, если все изображения обработаны
            break

    # Объединение всех собранныех признаки в один массив
    # (размером [число изображений, размер признака])
    X = np.vstack(features)

    # Проверка, что количество извлеченных признаков
    # совпадает с количеством строк в DataFrame
    if X.shape[0] != len(train_df):
        raise ValueError('Количество извлеченных признаков не\
        совпадает с количеством строк DataFrame.')

    # Копия исходного DataFrame, чтобы не изменять оригинал
    train_df = train_df.copy()

    # Новый столбец с векторами признаков
    # (каждый элемент — массив признаков для изображения)
    train_df[vector_column] = list(X)

    # Обновленный DataFrame
    return train_df

# Векторизация тренировочных изображений
train = create_image_vectors(train, train_gen_flow)

print('Размерность векоризованного train:', train['image_vector'].shape[0])
print('Длина вектора:', len(list(train['image_vector'].iloc[20])))

def create_image_vectors_test(test_df, generator, vector_column='image_vector'):
    """
    Создает модель внутри функции и извлекает признаки изображений из генератора,
    добавляя их в копию DataFrame как новый столбец.
    """

    base_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')
    total_samples = len(test_df)
    batch_size = generator.batch_size
    features = []

    for i in range(int(np.ceil(total_samples / batch_size))):
        batch_images = next(generator)  # предполагается, что возвращается только изображения
        batch_images = preprocess_input(batch_images)
        batch_features = base_model.predict(batch_images)
        features.append(batch_features)
        if len(features) * batch_size >= total_samples:
            break

    X = np.vstack(features)
    if X.shape[0] != total_samples:
        raise ValueError("Количество извлеченных признаков не совпадает с количеством строк DataFrame.")
    df_copy = test_df.copy()
    df_copy[vector_column] = list(X)
    return df_copy

# Векторизация тестовых изображений
test_images = create_image_vectors_test(test_images, test_gen_flow)

"""1/1 ━━━━━━━━━━━━━━━━━━━━ 13s 13s/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 6s 6s/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 3s 3s/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 5s 5s/step

## Объединение векторов

Подготовим данные для обучения: объединим векторы изображений и векторы текстов с целевой переменной.

<div style="border:ridge violet 5px; padding: 30px; border-radius: 15px;">
<h3> На заметку <a class="tocSkip"> </h3>


Данные имеют следующий вид:
- `train['image_vector']` — плотная матрица векторизированных изображений (np.ndarray), размер 2048
- `train['text_vector']` — плотная матрица векторизированных изображений (np.ndarray), размер 768
- `train['match']` — метки, длиной 5272

Теперь у нас есть векторизованный текст, изображения и целевые метки в виде массивов, длинна векторов которых совпадает. Чтобы применить их для обучения модели, нужно объединить в один признак
"""

# Объединение признаков в плотную матрицу
train['combine_vector'] = train.apply(
    lambda x: np.hstack((
        np.array(x['image_vector']),
        np.array(x['text_vector'])
    )), axis=1
)
print('Длина векторов train:', train['combine_vector'].iloc[0].shape)

"""# Обучение модели предсказания соответствия

Для обучения разделим датасет на тренировочную и тестовую выборки. Простое случайное разбиение не подходит: нужно исключить попадание изображения и в обучающую, и в тестовую выборки.
"""

# Объект GroupShuffleSplit для разбиения данных
# n_splits=1: деление данных один раз
gss = GroupShuffleSplit(n_splits=1, train_size=0.7, random_state=42)

# Выполняем разбиение:
# - groups: группирующая переменная train['image'],
# чтобы не разрывать группы при разбиении
train_indices, val_indices = next(
    gss.split(
        X=train['combine_vector'],
        y=train['match'],
        groups=train['image']         # группирующая переменная (например, чтобы не смешивать группы)
    )
)

# Получаем обучающую и тестовую выборки по индексам
X_train = train['combine_vector'].iloc[train_indices]
X_val = train['combine_vector'].iloc[val_indices]
y_train = train['match'].iloc[train_indices]
y_val = train['match'].iloc[val_indices]

"""<div style="border:ridge violet 5px; padding: 30px; border-radius: 15px;">
<h3> На заметку <a class="tocSkip"> </h3>


Создадим модель, которая покажет близость двух векторов. Модель должна принимать на вход конкатенированный вектор, состоящий из векторов описания и изображений, и предсказывать итоговую оценку экспертов.
Далее выберем метрику, по которой сравним точность различных моделей.

Какие метрики подходят для регрессии?
- MSE (Mean Squared Error) — среднеквадратичная ошибка. Хорошо чувствует к большим ошибкам.
- RMSE (Root Mean Squared Error) — корень из MSE. Интерпретируем в тех же единицах измерения.
- MAE (Mean Absolute Error) — средняя абсолютная ошибка. Более устойчива к выбросам.
- R² (коэффициент детерминации) — показывает долю вариации целевой переменной, объясненную моделью. Пказывает качество модели в процентах объясненной дисперсии.

Остановимся на `RMSE`, она дает понятное представление о среднем отклонении предсказаний от истинных значений. Получим метрику в тех же единицах измерения, что и целевая переменная. Далее обучим несколько моделей. В качестве базовых моделей обязательно нужно рассмотреть:

1. Линейную регрессию;
2. Полносвязные нейронные сети.

## LinearRegression
"""

lin_model = LinearRegression()
lin_model.fit(X_train.to_list(), y_train)

# Предсказания
preds = lin_model.predict(X_val.to_list())

# Делаем положительными значениями
rmse_scores = root_mean_squared_error(y_val, preds)
print("Средний RMSE на кросс-валидации:", np.mean(rmse_scores))

"""## Sequential"""

# Функция для создания нейронной сети для задачи регрессии
def create_nn_model(input_dim):
    # Создаем последовательную модель — слои идут один за другим
    model = Sequential()

    # Первый скрытый слой:
    # - 128 нейронов
    # - функция активации ReLU (Rectified Linear Unit)
    # - input_dim указывает размер входных данных (число признаков)
    model.add(Dense(128, activation='relu', input_dim=input_dim))

    # Слой Dropout:
    # - отключает 20% нейронов случайным образом во время обучения
    # - помогает предотвратить переобучение модели
    model.add(Dropout(0.2))

    # Второй скрытый слой:
    # - 64 нейрона
    # - функция активации ReLU
    model.add(Dense(64, activation='relu'))

    # Еще один слой Dropout для регуляризации
    model.add(Dropout(0.2))

    # Выходной слой:
    # - один нейрон, так как предсказываем одно числовое значение
    # - по умолчанию активатор linear (без функции активации)
    model.add(Dense(1))

    return model

# Создаем модель, передавая размерность входных данных (число признаков)
nn_model = create_nn_model(np.array(X_train.tolist()).shape[1])

# Компилируем модель:
# - loss='mean_squared_error' — функция потерь для регрессии
nn_model.compile(loss='mean_squared_error',
                 optimizer=Adam(learning_rate=0.00001),
                 metrics=['RootMeanSquaredError'])

# Преобразуем X_train и X_val в массивы с типом float32
X_train_np = np.array(X_train.tolist(), dtype=np.float32)
X_val_np = np.array(X_val.tolist(), dtype=np.float32)

# Обучаем модель на тренировочных данных:
history = nn_model.fit(X_train_np, y_train,
                       epochs=50,
                       batch_size=512,
                       validation_data=(X_val_np, y_val),
                       verbose=1)

# После обучения оцениваем модель на валидационной выборке:
# evaluate возвращает значение функции потерь и метрики (RMSE)
loss, rmse = nn_model.evaluate(X_val_np, y_val)

# Сохраняем модель после обучения
nn_model.save('my_model.keras')

# Выводим результат: RMSE на валидационной выборке
print(f"RMSE на валидационной выборке: {rmse:.4f}")

"""<div style="border:ridge violet 5px; padding: 30px; border-radius: 15px;">
<h3> На заметку <a class="tocSkip"> </h3>


Есть подозрения, что модели недообучаются, проверим на тесте

# Тестирование модели

Получим эмбеддинги для всех тестовых изображений из папки `test_images`, выберем случайные 10 запросов из файла `test_queries.csv` и для каждого запроса выведем наиболее релевантное изображение.

1. Лемматизация текста
2. Проверка по мешку слов, если есть печать дисклеймера
3. Векторизация теста
4. Векторизация тестовых изображений

Напишем функцию, которая принимает на вход текстовое описание, делает его векторизацию и возвращает картинку с максимальным значением метрики. Для этого получили векторы для всех тестовых изображений с помощью генератора test_gen_flow, выбрали случайные 10 запросов из файла test_queries и для каждого запроса нужно вывести наиболее релевантное изображение. Сравните визуально качество поиска.

### LinearRegression
"""

def display_images_lin(text_list, PATCH_IMAGES):
    """
    Объединённая функция для выбора случайных текстов, проверки на ограничения
    и отображения изображений по запросу.
    text_list (pd.Series или list): список или серия текстовых запросов.
    PATCH_IMAGES (str or Path): путь к папке с изображениями.
    """
    # 1. Выбираем 10 случайных образцов из текста
    sample_texts = pd.Series(text_list).sample(n=10).reset_index(drop=True)

    # 2. Лемматизация образцов параллельно
    with ProcessPoolExecutor() as executor:
        lemmatized_texts = list(executor.map(lemmatize_text, sample_texts))

    # 3. Проверка на юридические ограничения
    bag_words = set(bag_word_lim)
    clean_texts = []

    for idx, text in enumerate(lemmatized_texts, start=1):
        words = set(text.split())
        if words.intersection(bag_words):
            print(f"Изображение №{idx} недоступно в вашей стране в\
            соответствии с местным законодательством.")
        else:
            clean_texts.append(text)

    # 4. Обработка каждого "чистого" текста
    for text in clean_texts:
        # Получаем векторное представление текущего текста
        text_embed = get_vector_text(text)

        # Объединяем векторы изображений и текста,
        # создавая единый комбинированный вектор
        test_images['combined_vector'] = (
            test_images['image_vector']
            .apply(lambda x: np.concatenate((x, text_embed)))
        )

        # Прогоняем нашу модель предсказания качества изображений
        predictions = lin_model.predict(
            test_images['combined_vector'].tolist()
        )

        # Возвращаемся назад в датафрейм, добавляя прогнозы
        test_images['predictions'] = predictions

        # Берём верхние 5 изображений по качеству (по оценкам модели)
        top_df = (
            test_images
            .sort_values(by='predictions', ascending=False)
            .head(5)
        )

        # Собираем названия файлов и оценки верхних изображений
        top_images = list(top_df['image'])
        top_scores = list(top_df['predictions'])

        # Печать запроса и вывод изображений
        print('')
        print('ВАШ ЗАПРОС:', text)
        print('')

        # Создаём рисунок для демонстрации пяти лучших изображений
        fig = plt.figure(figsize=(15, 5))

        # Настройка оформления осей графика
        plt.rcParams['axes.edgecolor'] = 'black'
        plt.rcParams['axes.linewidth'] = 0

        # Цикл по каждому из лучших изображений
        for i in range(5):
            fig.add_subplot(1, 6, i + 1, title=round(top_scores[i], 2))
            # Конструируем полный путь к изображению
            image_path = Path(PATCH_IMAGES) / top_images[i]
            # Читаем файл изображения
            image = Image.open(image_path)
            # Выводим само изображение
            plt.imshow(image)
            # Скрываем оси координат
            plt.xticks([])
            plt.yticks([])

        plt.tight_layout()
        plt.show()

display_images_lin(
    test_queries.query_text,
    '/content/to_upload/test_images/'
)

"""### Sequential"""

from tensorflow.keras.models import load_model

# Основная функция обработки и отображения изображений
def display_images_nn(text_list, PATCH_IMAGES):
    """
    Объединённая функция для выбора случайных текстов, проверки на ограничения
    и отображения изображений по запросу.
    text_list (pd.Series или list): список или серия текстовых запросов.
    PATCH_IMAGES (str or Path): путь к папке с изображениями.
    """

    # 1. Выбираем 10 случайных образцов из текста
    sample_texts = pd.Series(text_list).sample(n=10).reset_index(drop=True)

    # 2. Лемматизация образцов параллельно
    with ProcessPoolExecutor() as executor:
        lemmatized_texts = list(executor.map(lemmatize_text, sample_texts))

    # 3. Проверка на юридические ограничения
    bag_words = set(bag_word_lim)
    clean_texts = []

    for idx, text in enumerate(lemmatized_texts, start=1):
        words = set(text.split())
        if words.intersection(bag_words):
            print(f"Изображение №{idx} недоступно в вашей стране\
            в соответствии с местным законодательством.")
        else:
            clean_texts.append(text)

    # 4. Обработка каждого чистого текста
    for text in clean_texts:
        # Получаем векторное представление текста
        text_embed = get_vector_text(text)

        # Комбинируем векторы изображений и текста
        test_images['combined_vector'] = (
            test_images['image_vector']
            .apply(lambda x: np.concatenate((x, text_embed)))
        )

        # Загружаем уже обученную модель
        nn_model = load_model('my_model.keras')

        # Прогоняем нейронную сеть на каждом запросе для оценки изображений
        predictions = nn_model.predict(
            np.array(test_images['combined_vector'].tolist())
        )

        # Добавляем прогнозы обратно в датафрейм
        test_images['predictions'] = predictions

        # Отбираем верхние 5 изображений по оценке
        top_df = test_images.sort_values(
            by='predictions',
            ascending=False
        ).head(5)

        # Получаем имена файлов и оценки верхних изображений
        top_images = list(top_df['image'])
        top_scores = list(top_df['predictions'])

        # Выводим запрос и изображения
        print('\nВаш запрос:', text)
        print('')

        # Рисуем графику с лучшими изображениями
        fig = plt.figure(figsize=(15, 5))

        # Настройки внешнего вида осей
        plt.rcParams['axes.edgecolor'] = 'black'
        plt.rcParams['axes.linewidth'] = 0

        # Выводим изображения
        for i in range(len(top_images)):
            fig.add_subplot(1, 6, i + 1, title=round(float(top_scores[i]), 2))
            image_path = Path(PATCH_IMAGES) / top_images[i]
            image = Image.open(image_path)
            plt.imshow(image)
            plt.xticks([])
            plt.yticks([])

        plt.tight_layout()
        plt.show()

# Запуск функции
display_images_nn(
    test_queries.query_text,
    '/content/to_upload/test_images/'
)

"""<div style="border:ridge violet 5px; padding: 30px; border-radius: 15px;">
<h3> На заметку <a class="tocSkip"> </h3>

**ВЫВОД**

1. **Исследовательский анализ.** Загрузили данные по ссылке и распаковали их архива. При просмотре данных они соответствуют техническому заданию и признаковому описанию.  Так же выделили оценки в экспертном датасете методом голосовании. Далее объединили их с удсординговыми для учета всех источников доверия. В итоге получили 1559 положительно оцененных пар с соответствием текст запроса - картинка и 4263 отрицательных оценок.
2. **Проверка данных.** Создали мешок слов, относящихся к запрету ранжирования изображений с детьми на основе местного законодательства. Далее лемматизировали мешок, текстовые запросы и фильтровали тренировочные данные. На выходе получили 5272 строк, что меньше на 10% от исходного количества образцов. Данный показатель находится в приемлемых рамках потери данных при предобработке.
3. **Векторизация текста.** Применили векторизацию текста запросов с использованием трансформера `BERT`,  разработанного Google. Получив при этом длину вектора запроса 768.
4. **Векторизация изображений.** Для векторизации изображений используем сверточную сеть `ResNet50`, предварительно обученную на наборе данных `ImageNet` с отключенными полносвязными слоями по предсказанию, так как нам это не нужно. Предварительно загрузили изображения с помощью генератора `ImageDataGenerator`
5. **Объединение векторов.** Следующим этапом объединили векторы изображений, длина 2048 и векторы текстов, длина 768. Общая размер 2816.
6. **Обучение модели предсказания соответствия.** Создали модель, которая покажет близость двух векторов. Остановились на метрике `RMSE`, она дает понятное представление о среднем отклонении предсказаний от истинных значений. Метрика в тех же единицах измерения, что и целевая переменная. Далее обучили `LinearRegression` с значением метрики на кросс-валидации 0.81 и `Sequential` с значением метрики на кросс-валидации 0.45.
7. **Тестирование модели.** Написали функцию, которая принимает на вход текстовое описание, делает его векторизацию и возвращает картинку с максимальным значением метрики. Для этого получили векторы для всех тестовых изображений с помощью генератора `test_gen_flow`, выбрали случайные 10 запросов из файла `test_queries` и для каждого запроса вывели наиболее релевантное изображение. Не смотря на более высокую метрику у `LinearRegression` ее результат не удовлетворительный. При анализе результатов запрос вообще не соответствует изображению, на лицо переобучение. С другой стороны у `Sequential` с низким значением метрики результат от части соответствует запросу.
"""